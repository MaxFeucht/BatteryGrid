{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YyrFvvoz_W8"
      },
      "source": [
        "## Project Reinforcement Learning\n",
        "\n",
        "Group 7 - Willem Huijzer, Joris Holshuijsen, Max Feucht\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lo4sRMQt2CMJ"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !rm -r BatteryGrid\n",
        "# !git clone https://github.com/MaxFeucht/BatteryGrid.git\n",
        "# !pip install -e BatteryGrid\n",
        "\n",
        "# import os\n",
        "# os.chdir('BatteryGrid')\n",
        "# !pip install -r requirements.txt\n",
        "# !pip install -e gym-env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Iez6NxK2z_W_"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import random\n",
        "\n",
        "from utils import RuleEvaluation, DDQNEvaluation, Plotter\n",
        "from agent import DDQNAgent, TemporalDDQNAgent\n",
        "\n",
        "from TestEnv import Electric_Car\n",
        "\n",
        "seed = 2705\n",
        "TRAIN = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2ssQ5boz_XC"
      },
      "source": [
        "### Setting up the environment:\n",
        "\n",
        "- 50kWh of usable capacity\n",
        "- 90% efficiency of the battery (both ways)\n",
        "- The maximum power available is 25kW (both ways - pertains to charging and discharging. One hour, we charge and discharge 25kWh)\n",
        "- Prices are in MWh! --> Rescale\n",
        "- Electricity is sold at the spot market price\n",
        "- Electricity is bought at twice the price to pay for the transmission costs and various taxes.\n",
        "- Only data up to the current time step available\n",
        "- Day: 8am to 6pm\n",
        "- Night: 6pm - 8am\n",
        "- 50% of days, car is not available during the day and returns with 20kWh less\n",
        "- Minimum capacity at 8am: 20kWh (if charge < 20kwH, charge from 7am to 8am)\n",
        "\n",
        "\n",
        "### Open Questions:\n",
        "\n",
        "- Intensity at which to sell or buy (Always 25kW, does it make sense to sell / buy less over a longer period of time)\n",
        "- What reward to give when trying to charge / sell during the day when car is absent. Do we give a penalty for even trying or do we let it do whatever it wants?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TPH-bgCtz_XC"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "\n",
        "def elongate(df):\n",
        "    df_long = pd.wide_to_long(df, i = \"PRICES\", j = \"hour\", stubnames=[\"Hour\"], sep = \" \").reset_index()\n",
        "    df_long.rename(columns={\"Hour\": \"price\", \"PRICES\": \"date\"}, inplace = True)\n",
        "    df_long['datetime'] = pd.to_datetime(df_long['date']) + pd.to_timedelta(df_long['hour'], unit='h')\n",
        "    df_long.sort_values(['datetime'], ascending=[True], inplace=True)\n",
        "    df_long['price'] = df_long['price'].astype(float) \n",
        "    return df_long.reset_index(drop=True)\n",
        "\n",
        "train_name = 'data/train.xlsx'\n",
        "val_name = 'data/validate.xlsx'\n",
        "\n",
        "train = elongate(pd.read_excel(train_name))\n",
        "val = elongate(pd.read_excel(val_name))\n",
        "\n",
        "features_train = pd.read_csv('data/features_train.csv')\n",
        "features_val = pd.read_csv('data/features_val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Feature Engineering\n",
        "# from feature_eng import *\n",
        "\n",
        "# gradient_sizes = [1, 2, 4, 6, 8, 12, 18, 24]\n",
        "# fourier_window = 72\n",
        "# window_sizes = [3, 6, 12, 24, 48, 72]\n",
        "\n",
        "# features_train = features_pipeline(train, fourier_window, gradient_sizes, window_sizes)\n",
        "# features_val = features_pipeline(val, fourier_window, gradient_sizes, window_sizes)\n",
        "\n",
        "# features_train.fillna(0, inplace=True)\n",
        "# features_val.fillna(0, inplace=True)\n",
        "\n",
        "# features_train = features_train.replace([np.inf, -np.inf], 0)\n",
        "# features_val = features_val.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# features_train.to_csv('data/features_train.csv', index=False)\n",
        "# features_val.to_csv('data/features_val.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzskV74jz_XE"
      },
      "source": [
        "## RULE BASED AGENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYgpghLgz_XE",
        "outputId": "caea8a08-efcd-49d8-9f18-3e7583b3d0e3"
      },
      "outputs": [],
      "source": [
        "# # Evaluate Rule-Based Agent\n",
        "# price_horizon = 24\n",
        "# df = train_name\n",
        "\n",
        "# rule_env = Electric_Car(path_to_test_data=df)\n",
        "# eval_rule = RuleEvaluation(env = rule_env, price_horizon=price_horizon)\n",
        "# eval_rule.evaluate(low_quantile = 0.25, high_quantile = 0.75, null_action = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RIvKPz8Iz_XG",
        "outputId": "7278eb66-52f3-4736-fabd-1c0f4139d1e4"
      },
      "outputs": [],
      "source": [
        "# # Visualize Rule-Based Agent\n",
        "# plot_range = (0, 100)\n",
        "\n",
        "# plotter = Plotter(eval_rule, range = plot_range)\n",
        "# plotter.plot_actions(battery = False, balance=False, absence = False)\n",
        "# plotter.plot_actions(battery = False, balance=False, absence = True)\n",
        "# plotter.plot_actions(battery = False, balance=True, absence = True)\n",
        "# plotter.plot_actions(battery = True, balance=True, absence = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCPLfvKSz_XH"
      },
      "source": [
        "## DDQN AGENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SN9XQ7M5z_XI"
      },
      "outputs": [],
      "source": [
        "seed = 2705\n",
        "rep = 1000000\n",
        "batch_size = 32\n",
        "gamma = 0.9\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 29999\n",
        "epsilon_min = 0.1\n",
        "learning_rate = 1e-4\n",
        "price_horizon = 96\n",
        "future_horizon = 0\n",
        "hidden_dim = 96\n",
        "action_classes = 5\n",
        "reward_shaping = True\n",
        "factor = 1\n",
        "verbose = False\n",
        "TRAIN = True\n",
        "df = train_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "7TdbCAoez_XJ",
        "outputId": "4a1c5916-c107-466c-f1d6-7ab660b66771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of engineered features:  49\n",
            "State dimension:  147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maxfeucht/Documents/VU/Project Reinforcement Learning/BatteryGrid/agent.py:79: RuntimeWarning: invalid value encountered in divide\n",
            "  return (var - np.min(var)) / (np.max(var) - np.min(var))\n",
            "  0%|          | 2/1000000 [00:00<17:28:08, 15.90it/s]0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "  0%|          | 1037/1000000 [00:05<1:07:24, 247.02it/s]ERROR:tornado.general:SEND Error: Host unreachable\n",
            "  0%|          | 2496/1000000 [00:13<1:31:49, 181.06it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     agent\u001b[38;5;241m.\u001b[39mreplay_memory\u001b[38;5;241m.\u001b[39madd_data((state, action, new_reward, t, new_state))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#Update DQN\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Update values\u001b[39;00m\n\u001b[1;32m     50\u001b[0m episode_balance \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n",
            "File \u001b[0;32m~/Documents/VU/Project Reinforcement Learning/BatteryGrid/agent.py:296\u001b[0m, in \u001b[0;36mDDQNAgent.optimize\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m#loss = F.mse_loss(action_q_values, targets.detach()) #Compute the loss between the predicted q-value for the action taken and the target q-value based on the next observation\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m#Gradient descent\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn_predict\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 296\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn_predict\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m#Switch DQN step\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/PRL/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/PRL/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if TRAIN:\n",
        "\n",
        "    # Initialize Environment\n",
        "    env = Electric_Car(path_to_test_data=df)\n",
        "\n",
        "\n",
        "    #Initialize DQN\n",
        "    agent = DDQNAgent(env = env,\n",
        "                      features = features_train,\n",
        "                      epsilon_decay = epsilon_decay,\n",
        "                      epsilon_start = epsilon,\n",
        "                      epsilon_end = epsilon_min,\n",
        "                      discount_rate = gamma,\n",
        "                      lr = learning_rate,\n",
        "                      buffer_size = 100000,\n",
        "                      price_horizon = price_horizon,\n",
        "                      hidden_dim=hidden_dim,\n",
        "                      action_classes = action_classes, \n",
        "                      verbose = verbose)\n",
        "\n",
        "    obs, r, terminated, _, _ = env.step(random.randint(-1,1)) # Reset environment and get initial observation\n",
        "    state, grads = agent.obs_to_state(obs)\n",
        "\n",
        "    \n",
        "    episode_balance = 0\n",
        "    episode_loss = 0\n",
        "    episode_counter = 0\n",
        "    episode_reward = 0\n",
        "\n",
        "    with tqdm(total=rep) as pbar:\n",
        "        for i in range(rep):\n",
        "\n",
        "            action = agent.choose_action(i, state, greedy = False) # Choose action (discrete)\n",
        "            cont_action = agent.action_to_cont(action) # Convert to continuous action\n",
        "            \n",
        "            new_obs, r, t, _, _ = env.step(cont_action)\n",
        "            new_state, new_grads = agent.obs_to_state(new_obs)\n",
        "            \n",
        "            # Reward Shapi            \n",
        "            new_reward = agent.shape_reward(r, cont_action, grads, apply = reward_shaping, factor = factor)\n",
        "\n",
        "            # Fill replay buffer - THIS IS THE ONLY THING WE DO WITH THE CURRENT OBSERVATION - LEARNING IS FULLY PERFORMED FROM THE REPLAY BUFFER\n",
        "            if state.shape[0] == agent.state_dim and new_state.shape[0] == agent.state_dim:\n",
        "                agent.replay_memory.add_data((state, action, new_reward, t, new_state))\n",
        "\n",
        "            #Update DQN\n",
        "            loss = agent.optimize(batch_size)\n",
        "            \n",
        "            # Update values\n",
        "            episode_balance += r\n",
        "            episode_reward += r\n",
        "            episode_loss += loss\n",
        "\n",
        "            # New observation\n",
        "            state = new_state\n",
        "            grads = new_grads # Gradients for reward shaping\n",
        "            \n",
        "            pbar.update(1)\n",
        "\n",
        "            if t:\n",
        "                # Reset Environment\n",
        "                env.counter = 0\n",
        "                env.hour = 1\n",
        "                env.day = 1\n",
        "                episode_counter += 1\n",
        "                print('Episode ', episode_counter, 'Balance: ', episode_balance, 'Reward: ', episode_reward, 'Loss: ', episode_loss) # Add both balance and reward to see how training objective and actually spent money differ\n",
        "                episode_loss = 0\n",
        "                episode_balance = 0\n",
        "                episode_reward = 0\n",
        "                \n",
        "                \n",
        "                if episode_counter % 4 == 0:\n",
        "                    # Evaluate DQN\n",
        "                    train_dqn = DDQNEvaluation(price_horizon = price_horizon)\n",
        "                    train_dqn.evaluate(agent = agent)\n",
        "                    \n",
        "                    # Reset Environment\n",
        "                    env.counter = 0\n",
        "                    env.hour = 1\n",
        "                    env.day = 1\n",
        "                    \n",
        "\n",
        "    # Save agent\n",
        "    torch.save(agent.dqn_predict.state_dict(), f'models/agent_{action_classes}_hdim{hidden_dim}.pt')\n",
        "    pbar.close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(agent.dqn_predict.state_dict(), f'models/agent_{action_classes}_hdim{hidden_dim}_newenv.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DMNcJ1Mkz_XK",
        "outputId": "16f0435f-7953-4271-f5a4-08a7e04085f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of engineered features:  49\n",
            "State dimension:  147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maxfeucht/Documents/VU/Project Reinforcement Learning/BatteryGrid/agent.py:79: RuntimeWarning: invalid value encountered in divide\n",
            "  return (var - np.min(var)) / (np.max(var) - np.min(var))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Initialize Environment\n",
        "env = Electric_Car(path_to_test_data=df)\n",
        "\n",
        "#Initialize DQN\n",
        "agent = DDQNAgent(env = env,\n",
        "                    features = features_train,\n",
        "                    epsilon_decay = epsilon_decay,\n",
        "                    epsilon_start = epsilon,\n",
        "                    epsilon_end = epsilon_min,\n",
        "                    discount_rate = gamma,\n",
        "                    lr = learning_rate,\n",
        "                    buffer_size = 100000,\n",
        "                    price_horizon = price_horizon,\n",
        "                    hidden_dim=hidden_dim,\n",
        "                    action_classes = action_classes)\n",
        "\n",
        "agent.dqn_predict.load_state_dict(torch.load(f'models/agent_{action_classes}_hdim{hidden_dim}.pt'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Absolute Balance:  -4258.088978919754\n"
          ]
        }
      ],
      "source": [
        "# Evaluate Rule-Based Agent\n",
        "df = train_name\n",
        "\n",
        "eval_ddqn = DDQNEvaluation(price_horizon=price_horizon)\n",
        "eval_ddqn.evaluate(agent = agent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IjZfnrkz_XM",
        "outputId": "b63576a4-a15d-4d28-da43-ba0e313bed40"
      },
      "outputs": [],
      "source": [
        "#Visualize DDQN Agent\n",
        "plot_range = [2200, 2400]\n",
        "\n",
        "plotter = Plotter(eval_ddqn, range = plot_range)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = False)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = True)\n",
        "plotter.plot_actions(battery = False, balance=True, absence = True)\n",
        "plotter.plot_actions(battery = True, balance=True, absence = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIX DATE RANGE FOR DDQN EVALUATION!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To Do Reward Shaping:\n",
        "\n",
        "Think about rules to check which rewards to penalize / increase\n",
        "\n",
        "Gradient-based Penalty Rules ####\n",
        "    \n",
        "- If gradient high and previous gradient high: Penalize selling\n",
        "- If gradient high and previous gradient low:\n",
        "\n",
        "\n",
        "Visualize, to which datapoints the rules will apply\n",
        "Implement rules in the reward shaping function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def penalty_func(reward, grad1, grad2, grad3, grad4, grad6, grad8, grad12, grad18, grad24):\n",
        "    \n",
        "    # Penalty for selling while price just started rising (recency is indicated by grad6 > grad3)\n",
        "    if grad1 > 0:\n",
        "        if grad6 < grad3:\n",
        "            penalty = max(grad1, grad2) # the higher the gradient, the higher the penalty\n",
        "        else:\n",
        "            penalty = max(grad1, grad2) # the higher the gradient, the higher the penalty\n",
        "    \n",
        "    # Penalty for selling while price is falling\n",
        "    #elif grad1 < 0:\n",
        "    #    penalty = np.abs(grad2) # the higher the gradient to the second to last point, the higher the penalty\n",
        "    \n",
        "    # No penalty \n",
        "    else:\n",
        "        penalty = 1 \n",
        "    \n",
        "    return np.abs(penalty)\n",
        "    \n",
        "    \n",
        "def penalty_func(reward, grad1, grad2, grad3, grad4, grad6, grad8, grad12, grad18, grad24):\n",
        "\n",
        "    if grad1 > 0:\n",
        "        penalty = max(grad1, grad2) # the higher the gradient, the higher the penalty\n",
        "    else:\n",
        "        penalty = 0\n",
        "        \n",
        "    if grad1 > 0 and grad2 > 0 and grad4 > 0 and grad6 > 0 and grad8 > 0 and grad12 > -reward/10 and grad18 > 0: #(grad6/grad4 > reward/(5*grad2)):\n",
        "        penalty = 0\n",
        "    \n",
        "    return np.abs(penalty)\n",
        "    \n",
        "# \n",
        "# if grad1 > 0 and grad2 > 0 and grad4 > 0 and grad6 > 0 and grad8 > 0 and grad12 > -1 :\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "range = (1100,1300)\n",
        "\n",
        "plt.figure(figsize=(15,5))        \n",
        "dates = features_train.iloc[range[0]:range[1],0]\n",
        "current_price = features_train['price'].iloc[range[0]:range[1]]\n",
        "penalized_reward = features_train[['price', 'gradient_1', 'gradient_2', 'gradient_3', 'gradient_6','gradient_4','gradient_8','gradient_12', 'gradient_18', 'gradient_24']].apply(lambda x: penalty_func(x.price, x.gradient_1, x.gradient_2, x.gradient_3,x.gradient_4, x.gradient_6, x.gradient_8, x.gradient_12, x.gradient_18, x.gradient_24), axis=1).iloc[range[0]:range[1]]\n",
        "scatter = plt.scatter(dates, current_price, c = penalized_reward, cmap = 'Reds')\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Action',rotation=270)\n",
        "plt.plot(dates, current_price, linestyle = '--', color = 'grey')\n",
        "var_names = ['Penalty', 'Price']\n",
        "plt.legend(var_names, loc = 'lower right')\n",
        "#plt.clim(0,1) \n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Penalty over Gradients\" + '\\n', size = 14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.343805421853684"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.log(77)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_range = [0, 100000]\n",
        "plotter = Plotter(eval_ddqn, range = plot_range)\n",
        "plotter.plot_single()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INVESTIGATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "price_horizon = 96\n",
        "hidden_dim = 64\n",
        "\n",
        "action = 0\n",
        "cont_action = 0\n",
        "\n",
        "for i in range(50):\n",
        "    \n",
        "    obs, reward, terminated, _, _ = agent.env.step(cont_action)\n",
        "    \n",
        "    old_action = action\n",
        "    old_q_values = q_values\n",
        "\n",
        "    print(\"Q-Values before update: \", round(old_q_values[old_action].detach().item(),3))\n",
        "    print(\"Reward: \", reward)\n",
        "\n",
        "    state, _ = agent.obs_to_state(obs)\n",
        "    state  = torch.as_tensor(state, dtype = torch.float32)\n",
        "    action = agent.choose_action(0, state, greedy = False)\n",
        "    cont_action = agent.action_to_cont(action)\n",
        "    q_values = agent.dqn_predict(torch.tensor(state))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcwdi-NIz_XN"
      },
      "source": [
        "# VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX_Eirr_z_XO",
        "outputId": "6cf6e560-3bb4-4f8a-a7c4-815e2135266c"
      },
      "outputs": [],
      "source": [
        "# Visualize DDQN Agent\n",
        "price_horizon = 96\n",
        "hidden_dim = 64\n",
        "df = val_name\n",
        "\n",
        "# Initialize Environment\n",
        "env = Electric_Car(path_to_test_data=df)\n",
        "\n",
        "#Initialize DQN\n",
        "agent = DDQNAgent(env = env,\n",
        "                    features = features_val,\n",
        "                    epsilon_decay = epsilon_decay,\n",
        "                    epsilon_start = epsilon,\n",
        "                    epsilon_end = epsilon_min,\n",
        "                    discount_rate = gamma,\n",
        "                    lr = learning_rate,\n",
        "                    buffer_size = 100000,\n",
        "                    price_horizon = price_horizon,\n",
        "                    hidden_dim=hidden_dim,\n",
        "                    action_classes = action_classes)\n",
        "\n",
        "agent.dqn_predict.load_state_dict(torch.load(f'models/agent_{action_classes}_hdim{hidden_dim}_newenv.pt'))\n",
        "\n",
        "\n",
        "# Evaluate Rule-Based Agent\n",
        "eval_ddqn = DDQNEvaluation(price_horizon=price_horizon)\n",
        "eval_ddqn.evaluate(agent = agent)\n",
        "\n",
        "#Visualize DDQN Agent\n",
        "plot_range = [3000, 3600]\n",
        "\n",
        "plotter = Plotter(eval_ddqn, range = plot_range)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = False)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = True)\n",
        "plotter.plot_actions(battery = False, balance=True, absence = True)\n",
        "plotter.plot_actions(battery = True, balance=True, absence = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0_9fyWuz_XQ",
        "outputId": "931b62d4-b078-4374-9f21-e614e7a870ab"
      },
      "outputs": [],
      "source": [
        "# Evaluate Rule-Based Agent\n",
        "price_horizon = 24\n",
        "df = val\n",
        "\n",
        "ddqn_env_val = gym.make('gym_env/BatteryGrid-v0', disable_env_checker=True)\n",
        "ddqn_env_val.setup(df, price_horizon=price_horizon, action_classes = 3)\n",
        "eval_ddqn_val = DDQNEvaluation(df, ddqn_env_val)\n",
        "eval_ddqn_val.evaluate(agent = agent)\n",
        "\n",
        "#Visualize DDQN Agent\n",
        "plot_range = [6500, 6700]\n",
        "\n",
        "plotter = Plotter(eval_ddqn_val, range = plot_range)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = False)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = True)\n",
        "plotter.plot_actions(battery = False, balance=True, absence = True)\n",
        "plotter.plot_actions(battery = True, balance=True, absence = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxPRr7BV8R6s"
      },
      "source": [
        "# TEMPORAL AGENT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2xkCOK648Pa2"
      },
      "outputs": [],
      "source": [
        "epsilon = 1\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 99999\n",
        "gamma = 0.99\n",
        "learning_rate = 5e-5\n",
        "lin_hidden_dim = 32\n",
        "temp_hidden_dim = 16\n",
        "kernel_size = 3\n",
        "dropout = 0.1\n",
        "action_classes = 5\n",
        "price_horizon = 120\n",
        "future_horizon = 0\n",
        "extra_penalty = False\n",
        "verbose = False\n",
        "TRAIN = True\n",
        "rep = 2000000\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "SlpOWCqh8bEf",
        "outputId": "7cda4f97-7d35-4b90-eb06-33e5847a000b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.setup to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.setup` for environment variables or `env.get_wrapper_attr('setup')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup with price horizon:  120  and future horizon:  0  and action space:  5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 26203/2000000 [04:17<5:44:18, 95.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  1 Balance:  -7872.259392918585 Reward:  -7872.259392918585 Loss:  364551.5186349079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 52384/2000000 [08:51<5:14:27, 103.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  2 Balance:  -7391.0569275228045 Reward:  -7391.0569275228045 Loss:  740323.4686083794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 78566/2000000 [13:45<5:46:56, 92.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  3 Balance:  -5187.653382293564 Reward:  -5187.653382293564 Loss:  296431.74337768555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 104747/2000000 [18:53<7:42:31, 68.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  4 Balance:  -2164.047174287446 Reward:  -2164.047174287446 Loss:  61720.66595456004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 130937/2000000 [24:08<5:39:48, 91.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  5 Balance:  -1640.1887859755586 Reward:  -1640.1887859755586 Loss:  19395.118714027107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 157122/2000000 [29:24<5:36:32, 91.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  6 Balance:  -1591.9041785640034 Reward:  -1591.9041785640034 Loss:  7180.21764674969\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 183299/2000000 [34:38<7:13:47, 69.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  7 Balance:  -1239.6413901954409 Reward:  -1239.6413901954409 Loss:  4170.589608972892\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 209483/2000000 [39:55<6:06:38, 81.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  8 Balance:  -1322.1944263822252 Reward:  -1322.1944263822252 Loss:  3672.9510052036494\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 235670/2000000 [45:13<5:56:13, 82.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  9 Balance:  -1246.4233044127761 Reward:  -1246.4233044127761 Loss:  2785.1822051750496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 244883/2000000 [47:05<5:37:28, 86.68it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-577b8772eaea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m#Update DQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemporal_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Update values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/BatteryGrid/agent.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Then: Compute DQN output for next state, and build the targets based on reward and the max q-value of the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mtarget_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_obs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Predict q-values for the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mmax_target_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Select the max q-value of the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m#new_rewards = torch.tensor(np.where(rewards < 0, rewards / 2, rewards)) # Penalize negative rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/BatteryGrid/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m         '''\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m#Temporal Branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice_horizon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice_horizon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Linear Branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/BatteryGrid/tcn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mtcn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Flatten over the features and timestep dimensions, preserve batch dimension (dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/BatteryGrid/tcn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/BatteryGrid/tcn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if TRAIN:\n",
        "\n",
        "    # Initialize Environment\n",
        "    env = gym.make('gym_env/BatteryGrid-v0', disable_env_checker=True)\n",
        "    env.setup(train, price_horizon=price_horizon, future_horizon=future_horizon, action_classes = action_classes, extra_penalty = extra_penalty)\n",
        "\n",
        "    #Initialize DQN\n",
        "    temporal_agent = TemporalDDQNAgent(env = env,\n",
        "                            device = device,\n",
        "                            epsilon_decay = epsilon_decay,\n",
        "                            epsilon_start = epsilon,\n",
        "                            epsilon_end = epsilon_min,\n",
        "                            discount_rate = gamma,\n",
        "                            lr = learning_rate,\n",
        "                            buffer_size = 100000,\n",
        "                            price_horizon = price_horizon,\n",
        "                            lin_hidden_dim = lin_hidden_dim,\n",
        "                            temp_hidden_dim = temp_hidden_dim,\n",
        "                            kernel_size = kernel_size,\n",
        "                            dropout = dropout,\n",
        "                            action_classes = action_classes)\n",
        "\n",
        "    obs, info = env.reset() # Reset environment and get initial observation\n",
        "\n",
        "    episode_balance = 0\n",
        "    episode_loss = 0\n",
        "    episode_counter = 0\n",
        "    episode_reward = 0\n",
        "\n",
        "    with tqdm(total=rep) as pbar:\n",
        "        for i in range(rep):\n",
        "\n",
        "            action = temporal_agent.choose_action(i, obs['tensor'], greedy = False)\n",
        "            new_obs,r,t,info = env.step(action)\n",
        "\n",
        "            # Fill replay buffer - THIS IS THE ONLY THING WE DO WITH THE CURRENT OBSERVATION - LEARNING IS FULLY PERFORMED FROM THE REPLAY BUFFER\n",
        "            temporal_agent.replay_memory.add_data((obs['tensor'], action, r, t, new_obs['tensor']))\n",
        "\n",
        "            #Update DQN\n",
        "            loss = temporal_agent.optimize(batch_size)\n",
        "\n",
        "            # Update values\n",
        "            episode_balance += info['balance']\n",
        "            episode_reward += r\n",
        "            episode_loss += loss\n",
        "\n",
        "            # New observation\n",
        "            obs = new_obs\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "            if i % 200000 == 0:\n",
        "              torch.save(temporal_agent.dqn_predict.state_dict(), f'models/temp_agent_{action_classes}_hdim{temp_hidden_dim}.pt')\n",
        "\n",
        "            if t:\n",
        "                obs, info = env.reset()\n",
        "                episode_counter += 1\n",
        "                print('Episode ', episode_counter, 'Balance: ', episode_balance, 'Reward: ', episode_reward, 'Loss: ', episode_loss) # Add both balance and reward to see how training objective and actually spent money differ\n",
        "                episode_loss = 0\n",
        "                episode_balance = 0\n",
        "                episode_reward = 0\n",
        "\n",
        "    # Save agent\n",
        "    torch.save(temporal_agent.dqn_predict.state_dict(), f'models/temp_agent_{action_classes}_hdim{temp_hidden_dim}.pt')\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgMdhcVQUHMJ",
        "outputId": "050cce82-f53b-4231-f075-e4100b4c5427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup with price horizon:  120  and future horizon:  0  and action space:  5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.setup to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.setup` for environment variables or `env.get_wrapper_attr('setup')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize Environment\n",
        "env = gym.make('gym_env/BatteryGrid-v0', disable_env_checker=True)\n",
        "env.setup(train, price_horizon=price_horizon, future_horizon=future_horizon, action_classes = action_classes, extra_penalty = extra_penalty)\n",
        "\n",
        "#Initialize DQN\n",
        "temporal_agent = TemporalDDQNAgent(env = env,\n",
        "                        device = device,\n",
        "                        epsilon_decay = epsilon_decay,\n",
        "                        epsilon_start = epsilon,\n",
        "                        epsilon_end = epsilon_min,\n",
        "                        discount_rate = gamma,\n",
        "                        lr = learning_rate,\n",
        "                        buffer_size = 100000,\n",
        "                        price_horizon = price_horizon,\n",
        "                        lin_hidden_dim = lin_hidden_dim,\n",
        "                        temp_hidden_dim = temp_hidden_dim,\n",
        "                        kernel_size = kernel_size,\n",
        "                        dropout = dropout,\n",
        "                        action_classes = action_classes)\n",
        "\n",
        "\n",
        "temporal_agent.dqn_predict.load_state_dict(torch.load(f'models/temp_agent_{action_classes}_hdim{temp_hidden_dim}.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WfNcEf_Bwdwk",
        "outputId": "61938bac-625a-4f1d-adbe-29108caca069"
      },
      "outputs": [],
      "source": [
        "## Training Evaluation of Temporal Agent:\n",
        "price_horizon = 120\n",
        "action_classes = 5\n",
        "df = train\n",
        "\n",
        "temp_env_train = gym.make('gym_env/BatteryGrid-v0', disable_env_checker=True)\n",
        "temp_env_train.setup(df, price_horizon=price_horizon, action_classes = action_classes)\n",
        "eval_temp_train = DDQNEvaluation(df, temp_env_train)\n",
        "eval_temp_train.evaluate(agent = temporal_agent)\n",
        "\n",
        "#Visualize DDQN Agent\n",
        "plot_range = [6500, 6700]\n",
        "\n",
        "plotter = Plotter(eval_temp_train, range = plot_range)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = False)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = True)\n",
        "plotter.plot_actions(battery = False, balance=True, absence = True)\n",
        "plotter.plot_actions(battery = True, balance=True, absence = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0AyFsfR8v7fa",
        "outputId": "ad92adf9-4c57-41c5-a229-127715efcb00"
      },
      "outputs": [],
      "source": [
        "## Validation of Temporal Agent:\n",
        "price_horizon = 120\n",
        "action_classes = 5\n",
        "df = val\n",
        "\n",
        "temp_env_val = gym.make('gym_env/BatteryGrid-v0', disable_env_checker=True)\n",
        "temp_env_val.setup(df, price_horizon=price_horizon, action_classes = action_classes)\n",
        "eval_temp_val = DDQNEvaluation(df, temp_env_val)\n",
        "eval_temp_val.evaluate(agent = temporal_agent)\n",
        "\n",
        "#Visualize DDQN Agent\n",
        "plot_range = [6500, 6700]\n",
        "\n",
        "plotter = Plotter(eval_temp_val, range = plot_range)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = False)\n",
        "plotter.plot_actions(battery = False, balance=False, absence = True)\n",
        "plotter.plot_actions(battery = False, balance=True, absence = True)\n",
        "plotter.plot_actions(battery = True, balance=True, absence = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un6C4gBez_XR"
      },
      "source": [
        "# Sanity Check of Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjneLYQUz_XR"
      },
      "outputs": [],
      "source": [
        "action_classes = 11\n",
        "test_env = gym.make('gym_env/BatteryGrid-v0', disable_env_checker=True)\n",
        "test_env.setup(df, price_horizon=price_horizon, extra_penalty = True, action_classes = action_classes, verbose = True)\n",
        "obs, info = test_env.reset()\n",
        "\n",
        "for k in range(4):\n",
        "    for i in reversed(range(action_classes)):\n",
        "        test_env.step(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1TZE6izz_XS"
      },
      "outputs": [],
      "source": [
        "# Reward Shaping:\n",
        "\n",
        "# when rewards are not sparse, a measure of the uncertainty on\n",
        "# the value function can be used to guide exploration.\n",
        "\n",
        "\n",
        "\n",
        "# Replay Buffer:\n",
        "\n",
        "# Problem: Our RL problem is a sequential one with a fixed starting point and a fixed order where to move. Does this affect the validity of s,a,r,s' for the replay buffer?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj1cDn4sz_XT"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering:\n",
        "\n",
        "# ⁠Gradient of curve\n",
        "# Value of to same date last year if available\n",
        "# ⁠Moving average on different scale\n",
        "# ⁠⁠Moving standard deviation  on different scales\n",
        "# ⁠⁠Season\n",
        "# ⁠Day of week\n",
        "# ⁠⁠Month\n",
        "# Average historic hourly price\n",
        "# Hardcoded price boundary (sell when price at previous timestep above threshold, buy when price at previous timestep is below threshold) - this can be a dynamic boundary (per month, per day, …) or a hard boundary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS45Wfb7z_XU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import weight_norm\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels) # num_channels is a list of the number of channels for each layer\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class TCNModel(nn.Module):\n",
        "    def __init__(self, seq_len, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TCNModel, self).__init__()\n",
        "        self.tcn = TemporalConvNet(\n",
        "            num_inputs, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.dense = nn.Linear(seq_len*num_channels[-1], 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tcn_output = self.tcn(x).flatten(end_dim = -2).t() #Flatten over the features and timestep dimensions, preserve batch dimension\n",
        "        return self.dense(self.dropout(tcn_output))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNM13VcNz_XU",
        "outputId": "ff4a4b65-113a-44f4-c429-202e3cde4cb1"
      },
      "outputs": [],
      "source": [
        "start = 6500\n",
        "range = 2000\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(train['datetime'][start:(start+range)], train['price'][start:(start+range)])\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FxHQZ7Hz_XV",
        "outputId": "39091fc4-1414-4e1e-b356-c27e62a5d003"
      },
      "outputs": [],
      "source": [
        "start = 2000\n",
        "range = 10000\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(val['datetime'][start:(start+range)], val['price'][start:(start+range)])\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ4WmsCFz_XV",
        "outputId": "23428a0c-b957-4499-cb97-abca3e253ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'battery': -0.5, 'prices': array([-0.76523653, -0.76929427, -0.85788829, -1.01614022, -1.07430118,\n",
            "       -1.17642101, -1.38336583, -1.59031066, -1.08985586, -1.04183925,\n",
            "       -0.96947619, -0.74089008, -0.43926462, -0.6421517 , -0.80716652,\n",
            "       -0.96947619, -0.99449893, -0.38719027, -0.12276077,  0.37498886,\n",
            "        0.18833275, -0.03957707,  0.60695643,  0.44261789, -0.10247207,\n",
            "       -0.47781316, -0.36487269, -0.7949933 , -0.9687999 , -0.84706765,\n",
            "       -0.59954541, -0.06865755,  0.29653919,  0.85244979,  1.17030622,\n",
            "        1.36440153,  1.52062458,  1.12499477,  0.99649962,  0.70569481,\n",
            "        0.86191786,  1.50980393,  2.16242404,  2.13266727,  1.41918104,\n",
            "        1.00123365,  1.25010847,  1.11958445]), 'hour': -0.4583333333333333, 'day': -0.4904109589041096, 'presence': 1, 'tensor': array([-0.5       , -0.76523653, -0.76929427, -0.85788829, -1.01614022,\n",
            "       -1.07430118, -1.17642101, -1.38336583, -1.59031066, -1.08985586,\n",
            "       -1.04183925, -0.96947619, -0.74089008, -0.43926462, -0.6421517 ,\n",
            "       -0.80716652, -0.96947619, -0.99449893, -0.38719027, -0.12276077,\n",
            "        0.37498886,  0.18833275, -0.03957707,  0.60695643,  0.44261789,\n",
            "       -0.10247207, -0.47781316, -0.36487269, -0.7949933 , -0.9687999 ,\n",
            "       -0.84706765, -0.59954541, -0.06865755,  0.29653919,  0.85244979,\n",
            "        1.17030622,  1.36440153,  1.52062458,  1.12499477,  0.99649962,\n",
            "        0.70569481,  0.86191786,  1.50980393,  2.16242404,  2.13266727,\n",
            "        1.41918104,  1.00123365,  1.25010847,  1.11958445, -0.45833333,\n",
            "       -0.49041096,  1.        ])}\n",
            "Current price: 0.039439999999999996, current hour: 1, current battery charge: 0, current presence: 1, current index: 48\n",
            "\n",
            "Action 0, Charging 27.77 kWh, balance: -2.1904975999999996\n",
            "\n",
            "Current price: 0.03198, current hour: 2, current battery charge: 24.993, current presence: 1, current index: 49\n",
            "\n",
            "Action 1, Charging 25 kWh, balance: -1.599\n",
            "\n",
            "Current price: 0.03, current hour: 3, current battery charge: 47.492999999999995, current presence: 1, current index: 50\n",
            "\n",
            "Action 2, Charging 20 kWh, balance: -1.2\n",
            "\n",
            "Current price: 0.02707, current hour: 4, current battery charge: 50.0, current presence: 1, current index: 51\n",
            "\n",
            "Action 3, Charging 15 kWh, balance: -0.8121\n",
            "\n",
            "Current price: 0.01918, current hour: 5, current battery charge: 50.0, current presence: 1, current index: 52\n",
            "\n",
            "Action 4, Charging 10 kWh, balance: -0.3836\n",
            "\n",
            "Current price: 0.01751, current hour: 6, current battery charge: 50.0, current presence: 1, current index: 53\n",
            "\n",
            "Action 5, Charging 5 kWh, balance: -0.1751\n",
            "\n",
            "Current price: 0.01703, current hour: 7, current battery charge: 50.0, current presence: 1, current index: 54\n",
            "\n",
            "Action 6, balance: 0\n",
            "\n",
            "Current price: 0.021, current hour: 8, current battery charge: 50.0, current presence: 1, current index: 55\n",
            "\n",
            "Current price: 0.02823, current hour: 9, current battery charge: 50.0, current presence: 0, current index: 56\n",
            "\n",
            "Current price: 0.03501, current hour: 10, current battery charge: 50.0, current presence: 0, current index: 57\n",
            "\n",
            "Current price: 0.04, current hour: 11, current battery charge: 50.0, current presence: 0, current index: 58\n",
            "\n",
            "Current price: 0.04418, current hour: 12, current battery charge: 50.0, current presence: 0, current index: 59\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/maxfeucht/anaconda3/envs/PRL/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.setup to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.setup` for environment variables or `env.get_wrapper_attr('setup')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/Users/maxfeucht/anaconda3/envs/PRL/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:189: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
            "  logger.warn(\n",
            "/Users/maxfeucht/anaconda3/envs/PRL/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:213: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/Users/maxfeucht/anaconda3/envs/PRL/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'numpy.float64'>\u001b[0m\n",
            "  logger.warn(\n",
            "/Users/maxfeucht/anaconda3/envs/PRL/lib/python3.11/site-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
            "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
            "/Users/maxfeucht/anaconda3/envs/PRL/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/Users/maxfeucht/anaconda3/envs/PRL/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:127: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method should be an int or np.int64, actual type: <class 'float'>\u001b[0m\n",
            "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n"
          ]
        }
      ],
      "source": [
        "price_horizon = 48\n",
        "future_horizon = 0\n",
        "\n",
        "env = gym.make('gym_env/BatteryGrid-v0')\n",
        "env.setup(val, price_horizon=price_horizon, future_horizon=future_horizon, verbose=True)\n",
        "obs,_ = env.reset()\n",
        "print(obs)\n",
        "\n",
        "obs,r,t,info = env.step(0)\n",
        "obs,r,t,info = env.step(1)\n",
        "obs,r,t,info = env.step(2)\n",
        "obs,r,t,info = env.step(3)\n",
        "obs,r,t,info = env.step(4)\n",
        "obs,r,t,info = env.step(5)\n",
        "obs,r,t,info = env.step(6)\n",
        "obs,r,t,info = env.step(7)\n",
        "obs,r,t,info = env.step(8)\n",
        "obs,r,t,info = env.step(9)\n",
        "obs,r,t,info = env.step(10)\n",
        "obs,r,t,info = env.step(11)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TeN_Ptkz_XX",
        "outputId": "ca3ca696-306d-4eeb-bc23-1e0067cbbbd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "action = 2\n",
        "kWh = (6 - action) * 5 # Discretize, such that action 0 means most discharge, i.e., kWh = (5 - 0)* 5 = 25\n",
        "kWh  -= 2.23 if action == 0 else 0 # Add charging loss\n",
        "\n",
        "print(kWh)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
